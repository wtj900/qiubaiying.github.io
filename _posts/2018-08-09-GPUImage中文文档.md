---
layout:     post
title:      GPUImage
subtitle:   中文文档
date:       2018-08-09
author:     JT
header-img: img/post-bg-github-cup.jpg
catalog: true
tags:
    - OpenGL
---


## 概要

GPUImage框架是一个获得BSD许可的iOS库，可让您将GPU加速滤镜和其他效果应用于图像，实时摄像机视频和电影。与Core Image（iOS 5.0的一部分）相比，GPUImage允许您编写自己的自定义过滤器，支持部署到iOS 4.0，并且具有更简单的界面。但是，它目前缺少核心图像的一些更高级的功能，例如面部检测。

对于像处理图像或实时视频帧这样的大规模并行操作，GPU比CPU具有一些显着的性能优势。在iPhone 4上，简单的图像过滤器在GPU上执行的速度比基于CPU的等效过滤器快100多倍。

但是，在GPU上运行自定义过滤器需要大量代码来设置和维护这些过滤器的OpenGL ES 2.0渲染目标。我创建了一个示例项目来执行此操作：

http://www.sunsetlakesoftware.com/2010/10/22/gpu-accelerated-video-processing-mac-and-ios

并发现我必须在其创建中编写很多样板代码。因此，我整理了这个框架，它包含了处理图像和视频时遇到的许多常见任务，并且使得您无需关心OpenGL ES 2.0基础。

在处理视频时，此框架与Core Image相比，在iPhone 4上只需2.5 ms即可从相机上传帧，应用伽玛滤波器和显示，而使用Core Image的相同操作则为106 ms。基于CPU的处理需要460毫秒，这使得GPUI在此硬件上的运行速度比Core Image快40倍，比CPU绑定处理速度快184倍。在iPhone 4S上，对于这种情况，GPUImage仅比Core Image快4倍，比CPU绑定处理快102倍。然而，对于更复杂的操作，如更大半径的高斯模糊，Core Image目前超过了GPUImage。

## 执照

BSD风格，License.txt中的框架提供完整许可证。

## 技术要求

* OpenGL ES 2.0：使用它的应用程序不能在原始的iPhone，iPhone 3G以及第一代和第二代iPod touch上运行
* iOS 4.1作为部署目标（4.0没有电影阅读所需的扩展）。 如果您希望在拍摄静态照片时显示实时视频预览，则需要使用iOS 4.3作为部署目标。
* iOS 5.0 SDK构建
* 设备必须配备相机才能使用与相机相关的功能（显然）
* 该框架使用自动引用计数（ARC），但如果添加为子项目，则应支持使用ARC和手动引用计数的项目，如下所述。 对于面向iOS 4.x的手动引用计数应用程序，您需要将-fobjc-arc添加到应用程序项目的Other Linker Flags中。

### 通用架构

GPUImage使用OpenGL ES 2.0着色器以比在CPU绑定例程中更快的速度执行图像和视频操作。但是，它隐藏了在简化的Objective-C界面中与OpenGL ES API交互的复杂性。此界面允许您定义图像和视频的输入源，在链中附加过滤器，并将生成的处理过的图像或视频发送到屏幕，UIImage或磁盘上的电影。

从源对象上载图像或视频帧，源对象是GPUImageOutput的子类。其中包括GPUImageVideoCamera（适用于iOS相机的实时视频），GPUImageStillCamera（用于使用相机拍摄照片），GPUImagePicture（适用于静态图像）和GPUImageMovie（适用于电影）。源对象将静态图像帧作为纹理上传到OpenGL ES，然后将这些纹理移交给处理链中的下一个对象。

链中的过滤器和其他后续元素符合GPUImageInput协议，该协议允许它们从链中的前一个链接接收提供或处理的纹理并对其执行某些操作。将链向下一步的对象视为目标，并且可以通过将多个目标添加到单个输出或过滤器来分支处理。

例如，从相机接收实时视频，将该视频转换为棕褐色调，然后在屏幕上显示视频的应用程序将设置如下所示的链：

```
GPUImageVideoCamera - > GPUImageSepiaFilter - > GPUImageView
```

## 将静态库添加到iOS项目中

注意：如果要在Swift项目中使用它，则需要使用“将其添加为框架”部分中的步骤而不是以下步骤。 Swift需要第三方代码的模块。

获得框架的最新源代码后，将其添加到应用程序中非常简单。首先将GPUImage.xcodeproj文件拖到应用程序的Xcode项目中，将框架嵌入到项目中。接下来，转到应用程序的目标并将GPUImage添加为目标依赖项。最后，您需要将libGPUImage.a库从GPUImage框架的Products文件夹拖到应用程序目标中的Link Binary With Libraries构建阶段。

GPUImage需要将一些其他框架链接到您的应用程序中，因此您需要在应用程序目标中添加以下链接库：

* CoreMedia
* corevideo的
* OpenGLES
* AVFoundation
* QuartzCore

您还需要找到框架标头，因此在项目的构建设置中，将标头搜索路径设置为从应用程序到GPUImage源目录中的框架/子目录的相对路径。使此标头搜索路径递归。

要在应用程序中使用GPUImage类，只需使用以下内容包含核心框架头：

```
#import“GPUImage.h”
```

请注意：如果在尝试使用Interface Builder构建界面时遇到错误“Interface Builder中的未知类GPUImageView”等，则可能需要在项目的构建设置中将-ObjC添加到其他链接器标志中。

此外，如果您需要将其部署到iOS 4.x，似乎当前版本的Xcode（4.3）要求您在最终应用程序中弱链接Core Video框架，或者您看到崩溃时出现“未找到符号”的消息：_CVOpenGLESTextureCacheCreate“当您创建用于上载到App Store或进行临时分发的存档时。为此，请转到项目的Build Phases选项卡，展开Link Binary With Libraries组，然后在列表中找到CoreVideo.framework。将列表最右侧的设置从Required更改为Optional。

此外，这是一个支持ARC的框架，因此如果您想在面向iOS 4.x的手动引用计数应用程序中使用它，您还需要将-fobjc-arc添加到您的其他链接器标志中。

### 在命令行构建静态库

如果您不希望将项目作为依赖项包含在应用程序的Xcode项目中，则可以为iOS模拟器或设备构建通用静态库。为此，请在命令行运行build.sh。生成的库和头文件将位于build / Release-iphone。您还可以通过更改build.sh中的IOSSDK_VER变量来更改iOS SDK的版本（可以使用xcodebuild -showsdks找到所有可用版本）。

## 将此作为框架（模块）添加到Mac或iOS项目中

Xcode 6和iOS 8支持使用完整框架，Mac也是如此，它简化了将其添加到应用程序的过程。要将其添加到您的应用程序中，我建议将.xcodeproj项目文件拖放到应用程序的项目中（就像在静态库目标中一样）。

对于您的应用程序，请转到其目标构建设置，然后选择“构建阶段”选项卡。在Tar​​get Dependencies分组下，在iOS上添加GPUImageFramework（不是GPUImage，它构建静态库）或Mac上的GPUImage。在Link Binary With Libraries部分下，添加GPUImage.framework。

这应该导致GPUImage构建为框架。在Xcode 6下，这也将构建为一个模块，允许您在Swift项目中使用它。如上设置，您只需要使用

```
import GPUImage
```

把它拉进来

然后，您需要添加新的“复制文件”构建阶段，将“目标”设置为“框架”，并将GPUImage.framework构建产品添加到该阶段。这将允许框架与您的应用程序捆绑在一起（否则，您将看到神秘的“dyld：Library not loaded：@ rpath / GPUImage.framework / GPUImage”执行时出错）。

### 文档

使用appledoc从标题注释生成文档。要构建文档，请切换到Xcode中的“文档”方案。您应确保“APPLEDOC_PATH”（用户定义的构建设置）指向appledoc二进制文件，可在Github上或通过Homebrew获得。它还将构建和安装.docset文件，您可以使用自己喜欢的文档工具查看该文件。

## 执行常见任务

### 过滤直播视频

要从iOS设备的相机中过滤实时视频，您可以使用以下代码：

```
GPUImageVideoCamera *videoCamera = [[GPUImageVideoCamera alloc] initWithSessionPreset:AVCaptureSessionPreset640x480 cameraPosition:AVCaptureDevicePositionBack];
videoCamera.outputImageOrientation = UIInterfaceOrientationPortrait;

GPUImageFilter *customFilter = [[GPUImageFilter alloc] initWithFragmentShaderFromFile:@"CustomShader"];
GPUImageView *filteredVideoView = [[GPUImageView alloc] initWithFrame:CGRectMake(0.0, 0.0, viewWidth, viewHeight)];

// Add the view somewhere so it's visible

[videoCamera addTarget:customFilter];
[customFilter addTarget:filteredVideoView];

[videoCamera startCameraCapture];
```

这将设置来自iOS设备的后置摄像头的视频源，使用预设尝试以640x480捕获。该视频是在界面处于纵向模式下捕获的，其中横向左侧安装的摄像机需要在显示之前旋转其视频帧。然后，使用CustomShader.fsh文件中的代码将自定义过滤器设置为来自摄像机的视频帧的目标。这些过滤的视频帧最终在UIView子类的帮助下显示在屏幕上，该子类可以呈现由此管道产生的过滤的OpenGL ES纹理。

可以通过设置其fillMode属性来更改GPUImageView的填充模式，这样，如果源视频的宽高比与视图的宽高比不同，则视频将被拉伸，以黑条为中心或缩放以填充。

对于混合滤镜和其他可以拍摄多个图像的滤镜，您可以创建多个输出并添加单个滤镜作为这两个输出的目标。将输出添加为目标的顺序将影响输入图像混合或以其他方式处理的顺序。

此外，如果您希望启用麦克风音频捕获以录制到电影，则需要将摄像机的audioEncodingTarget设置为电影编写器，如下所示：

```
videoCamera.audioEncodingTarget = movieWriter;
```

### 捕获和过滤静态照片

要捕获和过滤静态照片，您可以使用类似于过滤视频的过程。您使用GPUImageStillCamera而不是GPUImageVideoCamera：

```
stillCamera = [[GPUImageStillCamera alloc] init];
stillCamera.outputImageOrientation = UIInterfaceOrientationPortrait;

filter = [[GPUImageGammaFilter alloc] init];
[stillCamera addTarget:filter];
GPUImageView *filterView = (GPUImageView *)self.view;
[filter addTarget:filterView];

[stillCamera startCameraCapture];
```

这将为您提供静态相机预览视频的实时滤波馈送。请注意，此预览视频仅在iOS 4.3及更高版本中提供，因此如果您希望拥有此功能，则可能需要将其设置为部署目标。

想要捕获照片后，可以使用如下所示的回调块：

```
[stillCamera capturePhotoProcessedUpToFilter:filter withCompletionHandler:^(UIImage *processedImage, NSError *error){
    NSData *dataForJPEGFile = UIImageJPEGRepresentation(processedImage, 0.8);

    NSArray *paths = NSSearchPathForDirectoriesInDomains(NSDocumentDirectory, NSUserDomainMask, YES);
    NSString *documentsDirectory = [paths objectAtIndex:0];

    NSError *error2 = nil;
    if (![dataForJPEGFile writeToFile:[documentsDirectory stringByAppendingPathComponent:@"FilteredPhoto.jpg"] options:NSAtomicWrite error:&error2])
    {
        return;
    }
}];
```

上面的代码捕获由预览视图中使用的同一过滤器链处理的全尺寸照片，并将该照片作为JPEG保存在应用程序的文档目录中。

请注意，由于纹理大小的限制，该框架目前无法处理旧设备（iPhone 4S，iPad 2或Retina iPad之前的设备）宽度大于2048像素的图像。这意味着iPhone 4，其相机输出的照片仍然大于此照片，将无法捕捉到这样的照片。正在实施一种平铺机制来解决这个问题。所有其他设备应该能够使用此方法捕获和过滤照片。

### 处理静止图像

有两种方法可以处理静止图像并创建结果。您可以通过创建静态图像源对象并手动创建过滤器链来实现此目的的第一种方法：

```
UIImage *inputImage = [UIImage imageNamed:@"Lambeau.jpg"];

GPUImagePicture *stillImageSource = [[GPUImagePicture alloc] initWithImage:inputImage];
GPUImageSepiaFilter *stillImageFilter = [[GPUImageSepiaFilter alloc] init];

[stillImageSource addTarget:stillImageFilter];
[stillImageFilter useNextFrameForImageCapture];
[stillImageSource processImage];

UIImage *currentFilteredVideoFrame = [stillImageFilter imageFromCurrentFramebuffer];
```

请注意，对于从过滤器手动捕获图像，您需要设置-useNextFrameForImageCapture以告知过滤器您以后需要从中捕获它。默认情况下，GPUImage在过滤器中重用帧缓冲区以节省内存，因此如果您需要保留过滤器的帧缓冲区以进行手动图像捕获，则需要提前告知它。

对于要应用于图像的单个过滤器，您只需执行以下操作：

```
GPUImageSepiaFilter *stillImageFilter2 = [[GPUImageSepiaFilter alloc] init];
UIImage *quickFilteredImage = [stillImageFilter2 imageByFilteringImage:inputImage];
```

### 编写自定义过滤器

这个框架相对于iOS上的Core Image（从iOS 5.0开始）的一个显着优势是能够编写自己的自定义图像和视频处理过滤器。这些过滤器以OpenGL ES 2.0片段着色器的形式提供，使用类似C的OpenGL着色语言编写。

使用类似的代码初始化自定义过滤器

```
GPUImageFilter *customFilter = [[GPUImageFilter alloc] initWithFragmentShaderFromFile:@"CustomShader"];
```

用于片段着色器的扩展名为.fsh。此外，如果您不希望在应用程序包中发送片段着色器，则可以使用`-initWithFragmentShaderFromString：initializer`将片段着色器作为字符串提供。

片段着色器为要在该过滤器阶段呈现的每个像素执行计算。他们使用OpenGL着色语言（GLSL）来做到这一点，这是一种类似C语言，增加了特定于2D和3D图形的语言。片段着色器的一个示例是以下棕褐色调滤镜：

```
varying highp vec2 textureCoordinate;

uniform sampler2D inputImageTexture;

void main()
{
    lowp vec4 textureColor = texture2D(inputImageTexture, textureCoordinate);
    lowp vec4 outputColor;
    outputColor.r = (textureColor.r * 0.393) + (textureColor.g * 0.769) + (textureColor.b * 0.189);
    outputColor.g = (textureColor.r * 0.349) + (textureColor.g * 0.686) + (textureColor.b * 0.168);    
    outputColor.b = (textureColor.r * 0.272) + (textureColor.g * 0.534) + (textureColor.b * 0.131);
	outputColor.a = 1.0;

	gl_FragColor = outputColor;
}
```

对于可在GPUImage框架内使用的图像过滤器，需要纹理坐标变化的前两行（对于纹理中的当前坐标，标准化为1.0）和inputImageTexture一致（对于实际输入图像帧纹理） 。

着色器的其余部分抓取传入纹理中此位置处像素的颜色，以产生棕褐色调的方式对其进行处理，并将该像素颜色写出以用于处理的下一阶段管道。

将片段着色器添加到Xcode项目时需要注意的一点是，Xcode认为它们是源代码文件。要解决此问题，您需要手动将着色器从Compile Sources构建阶段移动到Copy Bundle Resources阶段，以便将着色器包含在应用程序包中。


![](https://wtj900.github.io/img/opengl/0001.png)


